"""library module with all training functions"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10, CIFAR100
import numpy as np
import pkbar
from architectures.cnn import CNN
from architectures.resnet_cifar import cifar_resnet
from architectures.resnet_tinyimagenet import tinyimg_resnet
from custom_modules.dataset import TensorDataset
from custom_modules.tinyimagenet_utils import get_tinyimagenet_data

#specifies the epochs on which the network weights should be saved
SAVE_EPOCHS = []


def save_model(path: str, file_name: str, net: nn.Sequential) -> None:
    """
    helper function which saves the given net in the specified path.
    if the path does not exists, it will be created.
    :param path: path where the model should be saved. Gets created if non-existent
    :param file_name: name as which the model should be saved
    :param net: object of the model

    :return: None
    """
    print("[ Saving Model ]")
    state = {
        'net': net.state_dict()
    }
    if not os.path.isdir('model_saves'):
        os.mkdir('model_saves')
    if not os.path.isdir('model_saves/'+str(path)):
        os.mkdir('model_saves/'+str(path))
    torch.save(state, "./model_saves/{}/{}".format(path, file_name))


def adjust_learning_rate(optimizer, epoch: int, epochs: int, learning_rate: int) -> None:
    """
    helper function to adjust the learning rate
    according to the current epoch to prevent overfitting.
    :paramo ptimizer: object of the used optimizer
    :param epoch: the current epoch
    :param epochs: the total epochs of the training
    :param learning_rate: the specified learning rate for the training

    :return: None
    """
    new_lr = learning_rate
    if epoch >= np.floor(epochs*0.5):
        new_lr /= 10
    if epoch >= np.floor(epochs*0.75):
        new_lr /= 10
    for param_group in optimizer.param_groups:
        param_group['lr'] = new_lr


def get_loaders(data_suffix: str, data_augmentation: bool, used_dataset: int,
                batch_size: int = 128) -> DataLoader:
    """
    helper function to create dataset loaders
    data_suffix is used to address custom dataset instead of the standard
    CIFAR10/CIFAR100/TinyImageNet datasets.
    :param data_suffix: the path of the dataset generated by an attack
    :param data_augmentation: flag specifies if data data_augmentation should be enabled
    :param used_dataset: specifies which dataset should be used if data_suffix is None.
                         0: CIFAR10, 1: CIFAR100, 2: TinyImageNet
    :param batch_size: batch size which should be used for the dataloader

    :return: dataloader with the specified dataset
    """
    #check if the cifar datasets have to be downloaded
    download_cifar10 = True
    download_cifar100 = True
    if os.path.isdir('./data/cifar-10-batches-py'):
        download_cifar10 = False
    if os.path.isdir('./data/cifar-100-python'):
        download_cifar100 = False
    if not os.path.isdir('./data/tiny-imagenet-200'):
        raise FileNotFoundError("Couldn't find TinyImageNet data at [./data/tiny-imagenet-200]")

    if data_augmentation:
        if used_dataset == 2:
            data_transform = transforms.Compose([transforms.RandomCrop(64, padding=8),
                                                 transforms.RandomHorizontalFlip(),
                                                 transforms.ToTensor()])
        elif used_dataset == 1:
            data_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
                                                 transforms.RandomHorizontalFlip(),
                                                 transforms.ToTensor()])
        else:
            data_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),
                                                 transforms.RandomHorizontalFlip(),
                                                 transforms.ToTensor()])
    else:
        data_transform = transforms.Compose([transforms.ToTensor()])

    # checks if a standard (origin) dataset or a custom dataset should be used
    # if data suffix is none, a non custom dataset will be used
    if data_suffix is None:
        if used_dataset == 2:
            train_data, train_labels, test_data, test_labels = get_tinyimagenet_data()
            train_dataset = TensorDataset(train_data, train_labels, transform=data_transform)
            test_dataset = TensorDataset(test_data, test_labels, transform=transforms.ToTensor())

        elif used_dataset == 1:
            train_dataset = CIFAR100(root='./data', train=True, download=download_cifar100,
                                     transform=data_transform)
            test_dataset = CIFAR100(root='./data', train=False, download=download_cifar100,
                                    transform=transforms.ToTensor())
        else:
            train_dataset = CIFAR10(root='./data', train=True, download=download_cifar10,
                                    transform=data_transform)
            test_dataset = CIFAR10(root='./data', train=False, download=download_cifar10,
                                   transform=transforms.ToTensor())

    #otherwise load the dataset from the given path
    else:
        data_path = "datasets/{}/".format(data_suffix)
        train_data = torch.load(os.path.join(data_path, "ims_{}".format(data_suffix))).to(device)
        train_labels = torch.load(os.path.join(data_path, "lab_{}".format(data_suffix))).to(device)

        if used_dataset == 2:
            _, _, test_data, test_labels = get_tinyimagenet_data()
            test_dataset = TensorDataset(test_data, test_labels, transform=transforms.ToTensor())

        elif used_dataset == 1:
            test_dataset = CIFAR100(root='./data', train=False, download=download_cifar100,
                                    transform=transforms.ToTensor())
        else:
            test_dataset = CIFAR10(root='./data', train=False, download=download_cifar10,
                                   transform=transforms.ToTensor())
        train_dataset = TensorDataset(train_data, train_labels, transform=data_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    return train_loader, test_loader


def get_model(is_resnet: bool, used_dataset: int, use_transfer: bool) -> nn.Sequential:
    """
    helper function to create and initialize the model.
    :param is_resnet: specifies if a resnet architecture should be used
    :param used_dataset: specifies num classes of the model's output layer.
                         0: CIFAR10, 1: CIFAR100, 2: TinyImageNet
    :return: returns the loaded model
    """
    if used_dataset == 2:
        num_classes = 200
    elif used_dataset == 1:
        num_classes = 100
    else:
        num_classes = 10

    if is_resnet:
        if use_transfer:
            net = models.resnet18(pretrained=True, progress=True)
        else:
            net = models.resnet18()
        net.fc = nn.Sequential(
            nn.Linear(512, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(4096, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(512, num_classes)
        )

    else:
        net = CNN(num_classes=num_classes)
    net = net.to(device)
    return net


def freeze_layers(model: nn.Sequential) -> nn.Sequential:
    """helper function to freeze feature extraction layers for transfer learning
    :param model: the network model
    :return: model with frozen layers
    """
    freeze_list = ["conv1", "bn1", "layer1", "layer2", "layer3", "layer4", "conv_layer"]

    for name, param in model.named_parameters():
        for layer in freeze_list:
            if layer in name:
                param.requires_grad = False

    return model


def init_classifier(layer) -> None:
    """
    helper function to apply re-initialization on the classifier weights
    :param layer: model layer on which the initialization should be applied

    :return: None
    """
    if isinstance(layer, nn.Linear):
        torch.nn.init.xavier_uniform_(layer.weight.data)


def init_transfer_learning(model: nn.Sequential):
    """
    initalizes the model and it's parameters for transfer learning. Therefore it uses a pretrained
    CIFAR100 model when CIFAR10 is the training dataset and vice versa.
    :param model: the model on which transfer learning should be initialized

    :return: model with preloaded weights and a freshly initialized classifier and it's parameters
    """
    model = model.to(device)
    model.apply(init_classifier)
    parameters = model.parameters()

    # freeze the convolutional layers
    #model = freeze_layers(model)
    #parameters = filter(lambda para: para.requires_grad, model.parameters())

    return model, parameters


def train(epochs: int, learning_rate: int, output_name: str, data_suffix: str,
          batch_size: int, device_name: str, is_resnet: bool, used_dataset: int,
          use_transfer: bool, data_augmentation: bool = True) -> None:
    """
    Main method to train the model with the specified parameters. Saves the model in every
    epoch specified in SAVE_EPOCHS. Prints the model status during the training.
    :param epochs: specifies how many epochs the training should last
    :param learning_rate: specifies the learning rate of the training
    :param output_name: the name as which the trained model should be saved
    :param data_suffix: the data_suffix of the custom dataset generated by the attack
    :param batch_size: specifies the batch size of the training data (default = 128)
    :param device_name: sets the device on which the training should be performed
    :param is_resnet: specifies if a resnet architecture should be used
    :param used_dataset: specifies which dataset should be used if data_suffix for a custom
                         dataset is None. 0: CIFAR10, 1: CIFAR100, 2: TinyImageNet
    :param use_transfer: flag if transfer learning should be used
    :param base_model: if transfer learning should be used, base_model specifies the model
                       which gets finetuned on the specified dataset by freezing all
                       of it's convolutional layers.
    :param data_augmentation: flag specifies if data augmentation should be used

    :return: None
    """
    print("[ Initialize Training ]")
    # create a global variable for the used device
    global device
    device = device_name

    # initializes the model, loss function, optimizer and dataset
    net = get_model(is_resnet, used_dataset, use_transfer)
    # if transfer learning flag is set, freeze certain layers and only pass the non frozen
    # layers to the optimizer
    if use_transfer:
        net, net_parameters = init_transfer_learning(model=net)
        epochs = np.ceil(epochs*0.25).astype(int)
        learning_rate = learning_rate*0.1
    else:
        net_parameters = net.parameters()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net_parameters, lr=learning_rate, momentum=0.9, weight_decay=5e-4)
    train_loader, test_loader = get_loaders(data_suffix, data_augmentation,
                                            used_dataset, batch_size)

    for epoch in range(0, epochs):
        # every epoch a new progressbar is created
        # also, depending on the epoch the learning rate gets adjusted before
        # the network is set into training mode
        kbar = pkbar.Kbar(target=len(train_loader)-1, epoch=epoch, num_epochs=epochs,
                          width=20, always_stateful=True)
        adjust_learning_rate(optimizer, epoch, epochs, learning_rate)
        net.train()
        correct = 0
        total = 0
        running_loss = 0.0

        # iterates over a batch of training data
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, targets)
            loss.backward()

            optimizer.step()
            _, predicted = outputs.max(1)

            # calculate the current running loss as well as the total accuracy
            # and update the progressbar accordingly
            running_loss += loss.item()
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            kbar.update(batch_idx, values=[("loss", running_loss/(batch_idx+1)),
                                           ("acc", 100. * correct / total)])
        # calculate the test accuracy of the network at the end of each epoch
        with torch.no_grad():
            net.eval()
            t_total = 0
            t_correct = 0
            for _, (inputs_t, targets_t) in enumerate(test_loader):
                inputs_t, targets_t = inputs_t.to(device), targets_t.to(device)
                outputs_t = net(inputs_t)
                _, predicted_t = outputs_t.max(1)
                t_total += targets_t.size(0)
                t_correct += predicted_t.eq(targets_t).sum().item()
            print("-> test acc: {}".format(100.*t_correct/t_total))

        # save the model at the end of the specified epochs as well as at
        # the end of the whole training
        if epoch in SAVE_EPOCHS:
            save_model(output_name, output_name+"_{}".format(epoch+1), net)
    save_model(output_name, output_name, net)

    # calculate the test accuracy of the network at the end of the training
    with torch.no_grad():
        net.eval()
        t_total = 0
        t_correct = 0
        for _, (inputs_t, targets_t) in enumerate(test_loader):
            inputs_t, targets_t = inputs_t.to(device), targets_t.to(device)
            outputs_t = net(inputs_t)
            _, predicted_t = outputs_t.max(1)
            t_total += targets_t.size(0)
            t_correct += predicted_t.eq(targets_t).sum().item()

    print("Final accuracy: Train: {} | Test: {}".format(100.*correct/total, 100.*t_correct/t_total))
